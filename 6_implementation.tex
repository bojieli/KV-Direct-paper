\section{Implementation}
\label{sec:implementation}

Our hardware platform is built on an Intel Stratix V FPGA based programmable NIC (\S\ref{sec:programmable-nic}).
The programmable NIC is attached to the server through two PCIe Gen3 x8 links in a bifurcated x16 physical connector, and contains 4 GiB of on-board DRAM with a single DDR3-1600 channel.

%Predominately, FPGAs are programmed with low-level hardware description languages.
For development efficiency, Intel FPGA SDK for OpenCL~\cite{aoc} as a high-level synthesis tool to generate hardware logic from OpenCL.
Our KV processor is implemented in 11K lines of OpenCL code and all kernels are fully pipelined, \ie, the throughput is one operation per clock cycle.
With 180~MHz clock frequency, our design can process KV operations at 180~M op/s if not bottlenecked by I/O.

%
%\begin{table}[t!]
%	\centering
%	\caption{Line of code and resource utilization of the KV processor.}
%	\label{tab:resource-util}
%	\scalebox{0.8}{
%		\begin{tabular}{l|r|r|r}
%			\toprule
%			Component & LoC & Logic (\%) & Memory (\%) \\
%			\midrule
%            Hash table & & & \\
%            Slab memory allocator & & & \\
%            Dynamic operation scheduler & & & \\
%            DRAM load dispatcher & & & \\
%            Operation decoder & & & \\
%            \midrule
%            Interconnect & & & \\
%            OpenCL runtime & & & \\
%            Network hard IP & & & \\
%            PCIe DMA and hard IP & & & \\
%            DDR controller & & & \\
%            \midrule
%            Total & & & \\
%			\bottomrule
%		\end{tabular}
%	}
%\end{table}


Below highlights two implementation details in the KV processor.


\textbf{Slab Memory Allocator.}

\begin{figure}[t]
\centering
\includegraphics[width=.5\textwidth,page=1]{slab.PNG}
\caption{Slab memory allocator.}
\label{fig:slab}
\vspace{-10pt}
\end{figure}

As shown in Figure~\ref{fig:slab}, for each slab size, the free slab pool is organized as a double-ended queue.
One end of the queue is accessed and cached by the NIC, the other end is accessed by the CPU.
Each slab cache is also a double-ended queue on the NIC, one end popped and pushed by the allocator and deallocator, the other end synced with host memory in batches.
Each double-ended queue has a high watermark and a low watermark.
When the host queue size grows above the high watermark, slab merging is triggered; when it drops below the low watermark, slab splitting is triggered.
The cache on NIC is synchronized with host DRAM in batches according to high and low watermarks.

%\textbf{Dynamic Operation Scheduler.}
%Upon simultaneous completion of an operation and arrival of a new operation, the reservation station prioritizes completion processing, because the completed operation has waited longer and prioritizing it benefits tail latency.
%During checking and execution of a hash slot, new completions may come.
%For simplicity, we finish scanning the chain of pending operations before moving to another key.

\textbf{DRAM Load Dispatcher.}
One technical challenge is the storage of metadata in DRAM cache, which requires additional 4 address bits and one dirty flag per 64-byte cache line.
Cache valid bit is not needed because all KVS storage is accessed exclusively by the NIC.
To store the 5 metadata bits per cache line, extending the cache line to 65 bytes would reduce DRAM performance due to unaligned access; saving the metadata elsewhere will double memory accesses.
Instead, we leverage spare bits in ECC DRAM for metadata storage.
ECC DRAM typically has 8 ECC bits per 64 bits of data.
For Hamming code to correct one bit of error in 64 bits of data, only 7 additional bits are required.
The 8th ECC bit is a parity bit for detecting double-bit errors.
As we access DRAM in 64-byte granularity and alignment, there are 8 parity bits per 64B data.
We increase the parity checking granularity from 64 data bits to 256 data bits, so double-bit errors can still be detected. This allows us to have 6 extra bits which can save our address bits and dirty flag.

%\textbf{KV Operation Decoder.}
%The KV operation decoder saves receive timestamp and UDP flow tuple as execution context in reservation station.
%When the KV operation completes, the flow control component measures the processing delay from the receive timestamp in execution context.
%The network encoder needs the flow tuple to generate completion packets and send to the client.
%To avoid head-of-line blocking, KV operations are processed out-of-order, so the client may receive KV completions in a different order from requests.
%The client could match the completions with requests by the key, because completions with a same key are guaranteed to be in request order.

\textbf{Vector Operation Decoder.}
%Throughout the KV processor design, we see batching as an universal principle in fetching multiple hash slots in one bucket, synchronizing free slab queues with host memory, lazy splitting and merging of slab slots, and data forwarding of dependent KV operations.
%Batching improves performance by amortizing control plane overhead to multiple data plane payloads.
Compared with PCIe, network is a more scarce resource with lower bandwidth (5~GB/s) and higher latency (2~$\mu$s).
%The control plane overhead of network is also larger. 
An RDMA write packet over Ethernet has 88 bytes of header and padding overhead, while a PCIe TLP packet has only 26 bytes of overhead.
This calls for network batching in two aspects: batching multiple KV operations in one packet and supporting vector operations for a more compact representation. Towards this end, we implement a decoder in the KV-engine to unpack multiple KV operations from a single RDMA packet.
%For computation efficiency, we could not use a general-purpose compression algorithm.
%Observing that many KVs have a same size or repetitive values, the KV format includes two flag bits to allow copying key and value size or the value part of the previous KV in the packet.
%For atomic and vector operations, additional fields include the opcode of the $\lambda$ function and the parameter to the active message.